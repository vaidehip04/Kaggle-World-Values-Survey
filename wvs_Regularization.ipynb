{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPython version:       7.8.0 (need at least 1.0)\n",
      "Numpy version:        1.16.5 (need at least 1.7.1)\n",
      "SciPy version:         1.3.1 (need at least 0.12.0)\n",
      "Pandas version:       0.25.1 (need at least 0.11.0)\n",
      "Matpltolib version:     3.1.1 (need at least 1.2.1)\n",
      "Scikit-Learn version: 0.21.3 (need at least 0.13.1)\n"
     ]
    }
   ],
   "source": [
    "#IPython is what you are using now to run the notebook\n",
    "import IPython\n",
    "print( \"IPython version:      %6.6s (need at least 1.0)\" % IPython.__version__)\n",
    "\n",
    "# Numpy is a library for working with arrays and matrices\n",
    "import numpy as np\n",
    "print( \"Numpy version:        %6.6s (need at least 1.7.1)\" % np.__version__)\n",
    "\n",
    "# SciPy implements many different numerical algorithms\n",
    "import scipy as sp\n",
    "print( \"SciPy version:        %6.6s (need at least 0.12.0)\" % sp.__version__)\n",
    "\n",
    "# Pandas makes working with data tables easier\n",
    "import pandas as pd\n",
    "print( \"Pandas version:       %6.6s (need at least 0.11.0)\" % pd.__version__)\n",
    "\n",
    "# Module for plotting\n",
    "import matplotlib.pyplot as plt  \n",
    "from pylab import *\n",
    "print( \"Matpltolib version:    %6.6s (need at least 1.2.1)\" %\n",
    "       matplotlib.__version__)\n",
    "%matplotlib inline\n",
    "# necessary for in-line graphics\n",
    "\n",
    "# SciKit Learn implements several Machine Learning algorithms\n",
    "import sklearn\n",
    "print(\"Scikit-Learn version: %6.6s (need at least 0.13.1)\" %\n",
    "       sklearn.__version__)\n",
    "import os\n",
    "# for certain system-related functions\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World Values Survey\n",
    "\n",
    "World Value Survey (WVS) is a large survey, conducted in many countries simultaneously. It revolves\n",
    "around public opinion about traditions, economy, politics, life and other things. I recommend you to\n",
    "consult the official questionnaire (uploaded in canvas/files/wvs).\n",
    "In this task the central question is V23: \"All things considered, how satisfied are you with your life as a\n",
    "whole these days?\" with answers ranging between 1 (completely dissatisfied) and 10 (completely satisfied).\n",
    "\n",
    "We are going to model this variables using linear regression.\n",
    "\n",
    "### 1 Explore and clean the data\n",
    "First, let's load data and take a closer look at it.\n",
    "\n",
    "1. Browse the WVS documentation and make sure you are familiar with coding of the variable V23.\n",
    "Note: you also have to consult the codebook to understand all the missings and how to remove those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of rows are 90350 and the number of columns are 328\n"
     ]
    }
   ],
   "source": [
    "# Reading the world Value data\n",
    "wvs_df = pd.read_csv('C:/Users/vaide/Documents/574-DS2/ps4/wvs.csv.bz2', sep ='\\t')\n",
    "\n",
    "#Exploring the data\n",
    "print(\"\\nThe number of rows are\", wvs_df.shape[0],\"and the number of columns are\",wvs_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load the data. Remove all the missing observations of V23. I mean all the missings, including the\n",
    "valid numeric codes that denote missing/invalid answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8  5  4  7  6  3 10  1  9 -1 -2  2 -5]\n"
     ]
    }
   ],
   "source": [
    "# Checking the unique values of V23\n",
    "print(wvs_df.V23.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8  5  4  7  6  3 10  1  9  2]\n"
     ]
    }
   ],
   "source": [
    "#Removing missing and invalid values by removing negative values and values greater than 10\n",
    "wvs_df = wvs_df[(wvs_df.V23 > 0) & (wvs_df.V23 < 11)]\n",
    "print(wvs_df.V23.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now make a table (or a plot) of different answers. What is the mean satisfaction level on this planet? How large a proportion of people are at 6 or more satisfied?\n",
    "\n",
    "(Note: without knowing more about how the sample was created, we should not talk about the\n",
    "planet. We should refer to \"respondents\" instead.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean satisfaction level of the respondents is 6.83\n",
      "\n",
      "The proportion of respondents satisfied at a 6 or higher level is 73.03 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Proportion_percentage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V23</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2828</td>\n",
       "      <td>3.150238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1985</td>\n",
       "      <td>2.211182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3463</td>\n",
       "      <td>3.857593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4600</td>\n",
       "      <td>5.124149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>11331</td>\n",
       "      <td>12.622116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>10666</td>\n",
       "      <td>11.881343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>15493</td>\n",
       "      <td>17.258357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>18213</td>\n",
       "      <td>20.288289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9264</td>\n",
       "      <td>10.319591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11928</td>\n",
       "      <td>13.287142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Count  Proportion_percentage\n",
       "V23                              \n",
       "1     2828               3.150238\n",
       "2     1985               2.211182\n",
       "3     3463               3.857593\n",
       "4     4600               5.124149\n",
       "5    11331              12.622116\n",
       "6    10666              11.881343\n",
       "7    15493              17.258357\n",
       "8    18213              20.288289\n",
       "9     9264              10.319591\n",
       "10   11928              13.287142"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting the response for each rating\n",
    "sat_life_df = pd.DataFrame(wvs_df.groupby('V23').V23.count())\n",
    "#Renaming the column\n",
    "sat_life_df.columns = [\"Count\"]\n",
    "#Creating the proportion percentage column\n",
    "sat_life_df[\"Proportion_percentage\"] = sat_life_df[\"Count\"] *100 / len(wvs_df)\n",
    "sat_more_than_5_df=sat_life_df[(sat_life_df.index>5)].sum()\n",
    "print(\"The mean satisfaction level of the respondents is\", round(wvs_df.V23.mean(),2))\n",
    "print(\"\\nThe proportion of respondents satisfied at a 6 or higher level is\",round(sat_more_than_5_df.Proportion_percentage,2),\"%\")\n",
    "sat_life_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Create the Design Matrix\n",
    "\n",
    "Now it is time to make the data suitable for a regression model. So far we have either used R-style formulas, or fed data into a ML model directly without much preparatory work. Now it is time to construct the design matrix manually. In case of linear regression, the design matrix is the data matrix that will be directly fed into the formula (X|^T.X)-1X|y, or any function that uses this or another similar formula. Design matrix can also be fed directly into other kind of models, such as logistic regression or decision tree. Design matrix is also needed by various libraries, in particular sklearn's LinearRegression, Ridge, and Lasso. There is an example of how to create a design matrix in the lecture notes, Section 2.1.7.\n",
    "\n",
    "Your task is to add the variables to the design matrix, one-by-one, and each time doing the necessary encoding if appropriate.\n",
    "\n",
    " Many variables are categorical. For instance, variable V2, country, is numeric with different numbers representing different countries. So in essence it is a categorical variable where categories are coded as numbers. The same is true for V80, most serious problem in the world. You should convert such variables to dummies (do your still remember pd.get_dummies?) and remove the original variable. But don't forget to remove missings!\n",
    "\n",
    " A large number of variables contain ordered values instead. For instance, V55 asks how much choice do you feel do you have over your life. The answers range from 1 (no choice at all) to 10 (a great deal of choice). We treat these as numeric response. Although, strictly speaking not correct, the model would be too messy if we were creating a category for each response. However, the missings (-5: inapplicable, -4: not asked etc) are not ordered in any meaningful sense. Hence your task is to\n",
    "remove missings.\n",
    "\n",
    "Note that many variables, e.g. v74b (important to help people nearby) and v90 (signing petition), contain a very large number of missings, and hence you essentially lose all your data if you include such variables. So you should remove such variables and replace with others that have more valid answers.\n",
    "\n",
    "Was it clear? Good. Enough of talk, let's dive into the real thing.\n",
    "\n",
    "1. Create your outcome variable y out of life satisfaction V23 (remove missings!)\n",
    "2. Create a design matrix that contains at least 100 variables from the WVS data. Your selected variables should contain at least a few categorical ones, such as V2 country. In each case:\n",
    "(a) remove missing observations\n",
    "(b) convert categorical variable to dummies if appropriate. Don't forget to drop the reference\n",
    "category.\n",
    "\n",
    "This will result in a large amount of code that is essentially copy-paste, but not exactly. It is a little\n",
    "bit tedious to do though. Think about options to make it more automatic but... hint: there are no\n",
    "good options...\n",
    "Note that when converting 100 variables into a design matrix, the latter may end up with many\n",
    "more columns.\n",
    "Keep in mind that you end up deleting quite a few observations, everything that contains missings in any\n",
    "of your selected variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response: \n",
    "\n",
    "The below code orders columns in relation to the number of the missing/negative values they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_pos_count_dict = {}\n",
    "for i in range(wvs_df.shape[1]):\n",
    "    column_pos_count_dict[wvs_df.iloc[:,i].name] = len(wvs_df[wvs_df.iloc[:,i]>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'V2': 89771,\n",
       " 'V23': 89771,\n",
       " 'V22': 89769,\n",
       " 'V13': 89766,\n",
       " 'V14': 89765,\n",
       " 'V16': 89763,\n",
       " 'V18': 89761,\n",
       " 'V17': 89760,\n",
       " 'V12': 89759,\n",
       " 'V44': 89759,\n",
       " 'V20': 89758,\n",
       " 'V15': 89756,\n",
       " 'V19': 89754,\n",
       " 'V21': 89754,\n",
       " 'V240': 89689,\n",
       " 'V242': 89602,\n",
       " 'V57': 89564,\n",
       " 'V11': 89503,\n",
       " 'V4': 89482,\n",
       " 'V5': 89300,\n",
       " 'V59': 89254,\n",
       " 'V10': 89124,\n",
       " 'V248': 88954,\n",
       " 'V84': 88878,\n",
       " 'V6': 88837,\n",
       " 'V80': 88755,\n",
       " 'V225': 88626,\n",
       " 'V200': 88557,\n",
       " 'V55': 88542,\n",
       " 'V9': 88523,\n",
       " 'V188': 88516,\n",
       " 'V82': 88507,\n",
       " 'V209': 88478,\n",
       " 'V208': 88472,\n",
       " 'V210': 88454,\n",
       " 'V102': 88443,\n",
       " 'V179': 88418,\n",
       " 'V211': 88377,\n",
       " 'V8': 88336,\n",
       " 'V45': 88286,\n",
       " 'V7': 88259,\n",
       " 'V143': 88240,\n",
       " 'V37': 88234,\n",
       " 'V39': 88234,\n",
       " 'V229': 88229,\n",
       " 'V202': 88223,\n",
       " 'V199': 88207,\n",
       " 'V191': 88198,\n",
       " 'V190': 88190,\n",
       " 'V189': 88047,\n",
       " 'V177': 88008,\n",
       " 'V250': 87994,\n",
       " 'V104': 87973,\n",
       " 'V170': 87955,\n",
       " 'V111': 87953,\n",
       " 'V113': 87877,\n",
       " 'V103': 87861,\n",
       " 'V79': 87850,\n",
       " 'V72': 87788,\n",
       " 'V140': 87786,\n",
       " 'V176': 87775,\n",
       " 'V100': 87705,\n",
       " 'V98': 87704,\n",
       " 'V78': 87645,\n",
       " 'V77': 87631,\n",
       " 'V24': 87553,\n",
       " 'V198': 87532,\n",
       " 'V214': 87487,\n",
       " 'V73': 87478,\n",
       " 'V238': 87424,\n",
       " 'V71': 87367,\n",
       " 'V83': 87340,\n",
       " 'V207': 87277,\n",
       " 'V205': 87247,\n",
       " 'V197': 87242,\n",
       " 'V56': 87239,\n",
       " 'V110': 87201,\n",
       " 'V52': 87195,\n",
       " 'V139': 87176,\n",
       " 'V75': 87119,\n",
       " 'V64': 87093,\n",
       " 'V192': 87092,\n",
       " 'V96': 87079,\n",
       " 'V108': 87038,\n",
       " 'V48': 87026,\n",
       " 'V62': 86996,\n",
       " 'V180': 86990,\n",
       " 'V70': 86980,\n",
       " 'V43': 86974,\n",
       " 'V41': 86972,\n",
       " 'V76': 86846,\n",
       " 'V99': 86841,\n",
       " 'V105': 86815,\n",
       " 'V193': 86813,\n",
       " 'V239': 86796,\n",
       " 'V115': 86790,\n",
       " 'V49': 86701,\n",
       " 'V133': 86659,\n",
       " 'V67': 86658,\n",
       " 'V134': 86658,\n",
       " 'V114': 86636,\n",
       " 'V60': 86621,\n",
       " 'V183': 86506,\n",
       " 'V227': 86505,\n",
       " 'V184': 86497,\n",
       " 'V68': 86483,\n",
       " 'V47': 86474,\n",
       " 'V213': 86451,\n",
       " 'V196': 86413,\n",
       " 'V131': 86234,\n",
       " 'V69': 86165,\n",
       " 'V121': 85982,\n",
       " 'V53': 85938,\n",
       " 'V137': 85933,\n",
       " 'V235': 85890,\n",
       " 'V219': 85866,\n",
       " 'V42': 85863,\n",
       " 'V36': 85859,\n",
       " 'V38': 85854,\n",
       " 'V142': 85818,\n",
       " 'V50': 85817,\n",
       " 'V51': 85815,\n",
       " 'V258A': 85710,\n",
       " 'V217': 85706,\n",
       " 'V220': 85622,\n",
       " 'V224': 85578,\n",
       " 'V117': 85521,\n",
       " 'V171': 85498,\n",
       " 'V218': 85434,\n",
       " 'V101': 85418,\n",
       " 'V138': 85383,\n",
       " 'V221': 85373,\n",
       " 'V204': 85339,\n",
       " 'V223': 85334,\n",
       " 'V194': 85281,\n",
       " 'V147': 85267,\n",
       " 'V65': 85220,\n",
       " 'V54': 85203,\n",
       " 'V222': 85194,\n",
       " 'V145': 85088,\n",
       " 'V152': 85081,\n",
       " 'V63': 84918,\n",
       " 'Y002': 84918,\n",
       " 'V136': 84813,\n",
       " 'V97': 84702,\n",
       " 'V119': 84644,\n",
       " 'V40': 84593,\n",
       " 'V150': 84463,\n",
       " 'V74': 84403,\n",
       " 'V109': 84380,\n",
       " 'V118': 84358,\n",
       " 'V181': 84334,\n",
       " 'V216': 84294,\n",
       " 'V132': 84275,\n",
       " 'V212': 84256,\n",
       " 'V61': 84145,\n",
       " 'V187': 84130,\n",
       " 'V178': 84089,\n",
       " 'V165': 84019,\n",
       " 'V172': 83690,\n",
       " 'V120': 83567,\n",
       " 'V226': 83561,\n",
       " 'V251': 83553,\n",
       " 'V201': 83545,\n",
       " 'V195': 83510,\n",
       " 'V81': 83445,\n",
       " 'V130': 83434,\n",
       " 'V237': 83421,\n",
       " 'V167': 83319,\n",
       " 'V160': 83293,\n",
       " 'V124': 83202,\n",
       " 'V106': 83058,\n",
       " 'V182': 83035,\n",
       " 'V146': 82982,\n",
       " 'V107': 82819,\n",
       " 'V164': 82766,\n",
       " 'V135': 82756,\n",
       " 'V203': 82649,\n",
       " 'V151': 82590,\n",
       " 'V122': 82496,\n",
       " 'V116': 82483,\n",
       " 'V46': 82336,\n",
       " 'V127': 81971,\n",
       " 'V141': 81866,\n",
       " 'V155': 81736,\n",
       " 'V158': 81730,\n",
       " 'V157': 81394,\n",
       " 'V186': 81383,\n",
       " 'V159': 81330,\n",
       " 'V153': 81148,\n",
       " 'V166': 81136,\n",
       " 'V173': 81108,\n",
       " 'V243': 81084,\n",
       " 'V128': 81075,\n",
       " 'V185': 81065,\n",
       " 'V66': 81052,\n",
       " 'V246': 80780,\n",
       " 'V123': 80719,\n",
       " 'V163': 80350,\n",
       " 'V168': 80297,\n",
       " 'V129': 80065,\n",
       " 'V154': 80062,\n",
       " 'V174': 79701,\n",
       " 'V249': 79497,\n",
       " 'V85': 79489,\n",
       " 'V126': 79233,\n",
       " 'V169': 79211,\n",
       " 'V148': 79037,\n",
       " 'V245': 78964,\n",
       " 'V255': 78758,\n",
       " 'V161': 78524,\n",
       " 'V252': 78384,\n",
       " 'V87': 78018,\n",
       " 'V112': 77702,\n",
       " 'V88': 77499,\n",
       " 'V156': 77486,\n",
       " 'V86': 77193,\n",
       " 'V234': 77055,\n",
       " 'V149': 76832,\n",
       " 'V206': 76385,\n",
       " 'V162': 76210,\n",
       " 'V175': 75112,\n",
       " 'V231': 74000,\n",
       " 'V233': 73857,\n",
       " 'V232': 73825,\n",
       " 'Y001': 73450,\n",
       " 'V253': 68983,\n",
       " 'V89': 68933,\n",
       " 'V95': 68709,\n",
       " 'V230': 68636,\n",
       " 'V236': 66565,\n",
       " 'V203A': 63728,\n",
       " 'V58': 62486,\n",
       " 'V228J': 58165,\n",
       " 'V228K': 58021,\n",
       " 'I_VOICE1': 54191,\n",
       " 'V228A': 52698,\n",
       " 'V228E': 51501,\n",
       " 'V228I': 51141,\n",
       " 'V228F': 50842,\n",
       " 'V228D': 50361,\n",
       " 'V228C': 50129,\n",
       " 'V228H': 49699,\n",
       " 'V228B': 49301,\n",
       " 'V228G': 49091,\n",
       " 'V207A': 41891,\n",
       " 'I_NORM1': 40891,\n",
       " 'V74B': 40456,\n",
       " 'V160B': 36423,\n",
       " 'V160C': 36362,\n",
       " 'V160F': 36339,\n",
       " 'V160D': 36220,\n",
       " 'V160H': 36187,\n",
       " 'V160I': 36128,\n",
       " 'V160G': 36048,\n",
       " 'V160A': 36021,\n",
       " 'V160E': 35672,\n",
       " 'V160J': 35428,\n",
       " 'V25': 29894,\n",
       " 'I_RELIGBEL': 27123,\n",
       " 'V26': 20275,\n",
       " 'V125_00': 16129,\n",
       " 'V215_01': 15656,\n",
       " 'V27': 15439,\n",
       " 'V90': 14918,\n",
       " 'V32': 13192,\n",
       " 'V28': 12935,\n",
       " 'V29': 12890,\n",
       " 'V31': 11549,\n",
       " 'V215_11': 11469,\n",
       " 'V34': 11162,\n",
       " 'V125_08': 10978,\n",
       " 'V215_12': 10614,\n",
       " 'V92': 10217,\n",
       " 'V125_07': 9984,\n",
       " 'V30': 9744,\n",
       " 'MN_228N': 8988,\n",
       " 'MN_228O': 8981,\n",
       " 'MN_228M': 8977,\n",
       " 'MN_228P': 8976,\n",
       " 'MN_228S7': 8971,\n",
       " 'MN_228S6': 8963,\n",
       " 'MN_228S3': 8944,\n",
       " 'MN_228S4': 8928,\n",
       " 'MN_163A': 8901,\n",
       " 'MN_228S1': 8872,\n",
       " 'MN_163C': 8860,\n",
       " 'V215_06': 8791,\n",
       " 'MN_228L': 8780,\n",
       " 'MN_228S8': 8618,\n",
       " 'V33': 8536,\n",
       " 'V35': 8294,\n",
       " 'MN_229A': 8251,\n",
       " 'MN_228Q': 8117,\n",
       " 'V125_15': 7914,\n",
       " 'MN_228S5': 7762,\n",
       " 'MN_249A1': 7442,\n",
       " 'MN_228R': 7322,\n",
       " 'MN_249A3': 6958,\n",
       " 'V93': 6706,\n",
       " 'V215_14': 6413,\n",
       " 'MN_230A': 6223,\n",
       " 'MN_233A': 6134,\n",
       " 'V215_13': 5642,\n",
       " 'V215_08': 5504,\n",
       " 'V125_09': 5470,\n",
       " 'V91': 5194,\n",
       " 'V125_12': 4672,\n",
       " 'V125_01': 4507,\n",
       " 'V94': 4262,\n",
       " 'V125_05': 4084,\n",
       " 'V125_06': 3667,\n",
       " 'V265': 3419,\n",
       " 'V215_16': 2094,\n",
       " 'V125_04': 1935,\n",
       " 'V215_10': 1706,\n",
       " 'V125_03': 1663,\n",
       " 'V215_18': 1566,\n",
       " 'V125_14': 1192,\n",
       " 'V215_03': 1178,\n",
       " 'V44_ES': 1168,\n",
       " 'V215_02': 1088,\n",
       " 'V125_11': 1074,\n",
       " 'V125_13': 1059,\n",
       " 'V215_15': 1051,\n",
       " 'V125_02': 785,\n",
       " 'V56_NZ': 772,\n",
       " 'MN_237B1': 202}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sorting according to the number of positive responses\n",
    "sorted_pos_count_dict = dict(sorted(column_pos_count_dict.items(), key=lambda item: item[1], reverse = True))\n",
    "sorted_pos_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code removes all missing observations from the selected columns including V23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting 105 varibles for design matrix\n",
    "top_var_list=list(sorted_pos_count_dict.keys())[:105] \n",
    "var_df=wvs_df[top_var_list]\n",
    "for i in range(var_df.shape[1]):\n",
    "     var_df = var_df[var_df.iloc[:,i] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the categorical variables V2 and V80 into dummies and dropping one dummy column to avoid the problem of perfect multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df.rename(columns={'V2':'country'}, inplace=True)\n",
    "var_df=pd.get_dummies(var_df, columns = ['country'])\n",
    "var_df = var_df.drop(\"country_887\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df.rename(columns={'V80':'problem'}, inplace=True)\n",
    "var_df=pd.get_dummies(var_df, columns = ['problem'])\n",
    "var_df = var_df.drop(\"problem_5\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create your outcome variable y out of life satisfaction V23 (remove missings!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res = var_df.V23  #This column is clean as we have already removed missing and invalid values\n",
    "design_matrix_df=var_df.drop(\"V23\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Condition numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compute the condition number of your output matrix in such a manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = design_matrix_df.columns\n",
    "array_new = list(columns[:1])\n",
    "columns = columns[1:]\n",
    "col_cond_num = {}\n",
    "for i in columns:\n",
    "    array_new.append(i)\n",
    "    col_cond_num[i] = np.linalg.cond(np.array(var_df[array_new]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our design matrix has 48888 rows and 158 columns.\n"
     ]
    }
   ],
   "source": [
    "print(\"Our design matrix has\",design_matrix_df.shape[0],\"rows and\",design_matrix_df.shape[1],\"columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The condition number after adding the V13 column is 4.59448341840832\n",
      "The condition number after adding the V14 column is 5.75185702638533\n",
      "The condition number after adding the V16 column is 6.584943179033798\n",
      "The condition number after adding the V18 column is 7.520200660827082\n",
      "The condition number after adding the V17 column is 8.358192772203937\n",
      "The condition number after adding the V12 column is 9.057570842826395\n",
      "The condition number after adding the V44 column is 11.354582828061679\n",
      "The condition number after adding the V20 column is 12.263481406976856\n",
      "The condition number after adding the V15 column is 13.195221854153317\n",
      "The condition number after adding the V19 column is 14.095042580999253\n",
      "The condition number after adding the V21 column is 14.754836638260839\n",
      "The condition number after adding the V240 column is 15.308114493523748\n",
      "The condition number after adding the V242 column is 118.3202382211488\n",
      "The condition number after adding the V57 column is 118.47377821186518\n",
      "The condition number after adding the V11 column is 118.6555949315956\n",
      "The condition number after adding the V4 column is 136.26814057341804\n",
      "The condition number after adding the V5 column is 138.43472214209743\n",
      "The condition number after adding the V59 column is 139.57065852268371\n",
      "The condition number after adding the V10 column is 140.25471106594202\n",
      "The condition number after adding the V248 column is 141.29549229805744\n",
      "The condition number after adding the V84 column is 141.508018247659\n",
      "The condition number after adding the V6 column is 141.6784684153321\n",
      "The condition number after adding the V225 column is 141.88736111966037\n",
      "The condition number after adding the V200 column is 142.1450156126447\n",
      "The condition number after adding the V55 column is 143.78774710171908\n",
      "The condition number after adding the V9 column is 145.01451596516083\n",
      "The condition number after adding the V188 column is 145.39258604497454\n",
      "The condition number after adding the V82 column is 145.60260052376364\n",
      "The condition number after adding the V209 column is 145.8909751713467\n",
      "The condition number after adding the V208 column is 146.07934131234865\n",
      "The condition number after adding the V210 column is 146.21077939522456\n",
      "The condition number after adding the V102 column is 151.1037790233909\n",
      "The condition number after adding the V179 column is 151.831031717615\n",
      "The condition number after adding the V211 column is 152.06993567101057\n",
      "The condition number after adding the V8 column is 152.61978047964553\n",
      "The condition number after adding the V45 column is 152.75473363211526\n",
      "The condition number after adding the V7 column is 152.9911929775643\n",
      "The condition number after adding the V143 column is 153.18928338290698\n",
      "The condition number after adding the V37 column is 162.29964819309288\n",
      "The condition number after adding the V39 column is 163.0309073502586\n",
      "The condition number after adding the V229 column is 163.40744275945974\n",
      "The condition number after adding the V202 column is 163.54111487310536\n",
      "The condition number after adding the V199 column is 163.773485977915\n",
      "The condition number after adding the V191 column is 164.10456807899294\n",
      "The condition number after adding the V190 column is 164.49866897653615\n",
      "The condition number after adding the V189 column is 164.93207588492504\n",
      "The condition number after adding the V177 column is 165.25003327807303\n",
      "The condition number after adding the V250 column is 165.37930501714587\n",
      "The condition number after adding the V104 column is 165.5285326231026\n",
      "The condition number after adding the V170 column is 165.65124777144175\n",
      "The condition number after adding the V111 column is 165.86207827276596\n",
      "The condition number after adding the V113 column is 166.0528678651825\n",
      "The condition number after adding the V103 column is 166.20684966268632\n",
      "The condition number after adding the V79 column is 166.46381072359156\n",
      "The condition number after adding the V72 column is 166.66213210487146\n",
      "The condition number after adding the V140 column is 169.05595953007688\n",
      "The condition number after adding the V176 column is 169.29926361662328\n",
      "The condition number after adding the V100 column is 169.86912731046277\n",
      "The condition number after adding the V98 column is 170.5900742270637\n",
      "The condition number after adding the V78 column is 170.7998803897675\n",
      "The condition number after adding the V77 column is 171.00976827462685\n",
      "The condition number after adding the V24 column is 171.11355454932854\n",
      "The condition number after adding the V198 column is 171.36438867607455\n",
      "The condition number after adding the V214 column is 171.43768040516912\n",
      "The condition number after adding the V73 column is 171.78846569940202\n",
      "The condition number after adding the V238 column is 172.17143496754866\n",
      "The condition number after adding the V71 column is 172.6508799401902\n",
      "The condition number after adding the V83 column is 196.98564801431374\n",
      "The condition number after adding the V207 column is 197.20105973147102\n",
      "The condition number after adding the V205 column is 197.9650151472372\n",
      "The condition number after adding the V197 column is 200.02203066252838\n",
      "The condition number after adding the V56 column is 201.29613629523345\n",
      "The condition number after adding the V110 column is 201.57920848270754\n",
      "The condition number after adding the V52 column is 201.9269592044546\n",
      "The condition number after adding the V139 column is 204.302862994793\n",
      "The condition number after adding the V75 column is 204.61289464129177\n",
      "The condition number after adding the V64 column is 204.74410166604216\n",
      "The condition number after adding the V192 column is 206.95751572678228\n",
      "The condition number after adding the V96 column is 208.0593116020493\n",
      "The condition number after adding the V108 column is 208.2174734869922\n",
      "The condition number after adding the V48 column is 208.3119258918597\n",
      "The condition number after adding the V62 column is 208.46756886124274\n",
      "The condition number after adding the V180 column is 209.30180406464217\n",
      "The condition number after adding the V70 column is 209.57037701850845\n",
      "The condition number after adding the V43 column is 209.6786857169879\n",
      "The condition number after adding the V41 column is 209.81183346987893\n",
      "The condition number after adding the V76 column is 210.3178986523713\n",
      "The condition number after adding the V99 column is 210.84791984636755\n",
      "The condition number after adding the V105 column is 211.21102239580838\n",
      "The condition number after adding the V193 column is 213.45682909956903\n",
      "The condition number after adding the V239 column is 214.35161220733966\n",
      "The condition number after adding the V115 column is 214.58181916900617\n",
      "The condition number after adding the V49 column is 214.68024745100988\n",
      "The condition number after adding the V133 column is 217.01066121353134\n",
      "The condition number after adding the V67 column is 217.22583669248635\n",
      "The condition number after adding the V134 column is 218.98965231864534\n",
      "The condition number after adding the V114 column is 219.21436311239032\n",
      "The condition number after adding the V60 column is 219.3429880657304\n",
      "The condition number after adding the V183 column is 219.5511442142528\n",
      "The condition number after adding the V227 column is 219.65542447915263\n",
      "The condition number after adding the V184 column is 219.80388470418217\n",
      "The condition number after adding the country_12 column is 615.4768431996505\n",
      "The condition number after adding the country_31 column is 615.5333532880503\n",
      "The condition number after adding the country_36 column is 615.536338280835\n",
      "The condition number after adding the country_51 column is 615.814547852863\n",
      "The condition number after adding the country_76 column is 615.9613197130734\n",
      "The condition number after adding the country_112 column is 615.9640132496296\n",
      "The condition number after adding the country_152 column is 616.167263748652\n",
      "The condition number after adding the country_156 column is 616.5809731734025\n",
      "The condition number after adding the country_158 column is 616.6480081259673\n",
      "The condition number after adding the country_170 column is 616.9473685248661\n",
      "The condition number after adding the country_196 column is 616.979430254115\n",
      "The condition number after adding the country_233 column is 617.0637116509422\n",
      "The condition number after adding the country_268 column is 617.2659799747248\n",
      "The condition number after adding the country_275 column is 618.6930848094008\n",
      "The condition number after adding the country_276 column is 619.1224198763855\n",
      "The condition number after adding the country_288 column is 619.5623857214576\n",
      "The condition number after adding the country_356 column is 620.7144619952908\n",
      "The condition number after adding the country_368 column is 623.0912586491977\n",
      "The condition number after adding the country_392 column is 638.2241936886118\n",
      "The condition number after adding the country_398 column is 643.9174620649882\n",
      "The condition number after adding the country_400 column is 644.6233535377413\n",
      "The condition number after adding the country_410 column is 655.1809769440284\n",
      "The condition number after adding the country_417 column is 661.2813033192778\n",
      "The condition number after adding the country_422 column is 666.0068294136238\n",
      "The condition number after adding the country_434 column is 672.3729890023392\n",
      "The condition number after adding the country_458 column is 679.8489061171322\n",
      "The condition number after adding the country_484 column is 686.4605849363028\n",
      "The condition number after adding the country_504 column is 702.8784781404975\n",
      "The condition number after adding the country_528 column is 713.2257427848722\n",
      "The condition number after adding the country_566 column is 716.1272945613441\n",
      "The condition number after adding the country_586 column is 723.833258648819\n",
      "The condition number after adding the country_604 column is 735.6127167171422\n",
      "The condition number after adding the country_608 column is 745.4552789091027\n",
      "The condition number after adding the country_616 column is 759.4605889048029\n",
      "The condition number after adding the country_634 column is 781.3046185777963\n",
      "The condition number after adding the country_642 column is 802.892482972662\n",
      "The condition number after adding the country_643 column is 818.7703709056223\n",
      "The condition number after adding the country_646 column is 835.2370369942131\n",
      "The condition number after adding the country_702 column is 867.7213628029591\n",
      "The condition number after adding the country_705 column is 891.3483598953964\n",
      "The condition number after adding the country_710 column is 946.3279975254151\n",
      "The condition number after adding the country_716 column is 1015.0166423800564\n",
      "The condition number after adding the country_724 column is 1044.3325444968798\n",
      "The condition number after adding the country_752 column is 1086.0215497197994\n",
      "The condition number after adding the country_764 column is 1145.6398422346122\n",
      "The condition number after adding the country_780 column is 1208.465064987478\n",
      "The condition number after adding the country_788 column is 1296.46532599316\n",
      "The condition number after adding the country_792 column is 1450.2651899403684\n",
      "The condition number after adding the country_804 column is 1661.266233275444\n",
      "The condition number after adding the country_840 column is 2377.6942130592865\n",
      "The condition number after adding the country_858 column is 2845.041675205679\n",
      "The condition number after adding the country_860 column is 4592.739038863418\n",
      "The condition number after adding the problem_1 column is 4593.497099563239\n",
      "The condition number after adding the problem_2 column is 4594.442057003917\n",
      "The condition number after adding the problem_3 column is 4595.393914478305\n",
      "The condition number after adding the problem_4 column is 4595.663117943389\n"
     ]
    }
   ],
   "source": [
    "for each_col in col_cond_num:\n",
    "    print(\"The condition number after adding the\" , each_col , \"column is\", col_cond_num[each_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final design matrix has a decent condition number approximately equal to 4596 and is well-suitable for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Do Some Social Science\n",
    "\n",
    "Before getting further, let's do a simple social science analysis. How is life satisfaction related to health (v11), perceived control over life (v55 ) and financial situation (v59 )? Let's analyze association between satisfaction and just these three variables.\n",
    "\n",
    "1. run a linear regression models explaining satisfaction with these three variables. Present the output table. I recommend to use statsmodels.formula.api for this task (but you have to use sklearn later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "soc_science_df = wvs_df[['V23','V11','V55','V59']]\n",
    "soc_science_df = soc_science_df[soc_science_df.V11 > 0]\n",
    "soc_science_df = soc_science_df[soc_science_df.V55 > 0]\n",
    "soc_science_df = soc_science_df[soc_science_df.V59 > 0]\n",
    "\n",
    "train_data = soc_science_df[:70391]\n",
    "test_data = soc_science_df[70391:]\n",
    "  \n",
    "regr = smf.ols(formula='V23~V11+V55+V59',data=train_data)\n",
    "fitmod=regr.fit() #fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. comment the output table in terms of relative effect size and statistical significance. Any surprises for you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response:\n",
    "\n",
    "From the below model summary we can see that all 3 response variables are statistically significant as the p-values are lesser than 0.05 and also the model has a large F-statistic value which means that the model is statistically significant. The model shows that when the variable V11 decreases in value (i.e. the state of health is very good), the life statisfaction increases. Similarly, when the financial situation of the respondents and their peceived control over life increases, the life satisfaction increases. This model also has a condition number of 50 which means that our training data is well-suited for modelling.\n",
    "\n",
    "However, the R-squared value for the model is 0.32 which is a bit low and it implies that the model cannot explain the variance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>V23</td>       <th>  R-squared:         </th>  <td>   0.320</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.320</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>1.104e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 06 Mar 2020</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:17:14</td>     <th>  Log-Likelihood:    </th> <td>-1.4427e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 70391</td>      <th>  AIC:               </th>  <td>2.886e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 70387</td>      <th>  BIC:               </th>  <td>2.886e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    3.7572</td> <td>    0.036</td> <td>  104.743</td> <td> 0.000</td> <td>    3.687</td> <td>    3.827</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V11</th>       <td>   -0.3958</td> <td>    0.009</td> <td>  -46.125</td> <td> 0.000</td> <td>   -0.413</td> <td>   -0.379</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V55</th>       <td>    0.2469</td> <td>    0.003</td> <td>   73.055</td> <td> 0.000</td> <td>    0.240</td> <td>    0.253</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V59</th>       <td>    0.3573</td> <td>    0.003</td> <td>  115.334</td> <td> 0.000</td> <td>    0.351</td> <td>    0.363</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>3306.564</td> <th>  Durbin-Watson:     </th> <td>   1.687</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5618.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>-0.392</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 4.141</td>  <th>  Cond. No.          </th> <td>    50.8</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    V23   R-squared:                       0.320\n",
       "Model:                            OLS   Adj. R-squared:                  0.320\n",
       "Method:                 Least Squares   F-statistic:                 1.104e+04\n",
       "Date:                Fri, 06 Mar 2020   Prob (F-statistic):               0.00\n",
       "Time:                        16:17:14   Log-Likelihood:            -1.4427e+05\n",
       "No. Observations:               70391   AIC:                         2.886e+05\n",
       "Df Residuals:                   70387   BIC:                         2.886e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      3.7572      0.036    104.743      0.000       3.687       3.827\n",
       "V11           -0.3958      0.009    -46.125      0.000      -0.413      -0.379\n",
       "V55            0.2469      0.003     73.055      0.000       0.240       0.253\n",
       "V59            0.3573      0.003    115.334      0.000       0.351       0.363\n",
       "==============================================================================\n",
       "Omnibus:                     3306.564   Durbin-Watson:                   1.687\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5618.192\n",
       "Skew:                          -0.392   Prob(JB):                         0.00\n",
       "Kurtosis:                       4.141   Cond. No.                         50.8\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitmod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. compute and present RMSE (just on training data). This will serve as the benchmark for the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on the training dataset is 1.8788670435909323\n"
     ]
    }
   ],
   "source": [
    "print(\"The RMSE on the training dataset is\",sqrt(fitmod.mse_resid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Back to ML: Model\n",
    "\n",
    "Now it is time to use all these variables to model satisfaction. Use sklearn.linear_model.LinearRegression here as this is easy to be switched with ridge and lasso, and it takes in the design matrix directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. compute the condition number for your design matrix (just a single number, not the stepwise procedure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number of X is : 4595.663117943389\n"
     ]
    }
   ],
   "source": [
    "print(\"Condition number of X is :\", np.linalg.cond(design_matrix_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Split the data into training-validation chunks (80-20 or so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing data \n",
    "X_train, X_test, y_train, y_test = train_test_split(design_matrix_df, y_res, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. compute the condition number for your training design matrix (just a single number, not the stepwise procedure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number of X-train is : 4630.745564967672\n"
     ]
    }
   ],
   "source": [
    "print(\"Condition number of X-train is :\", np.linalg.cond(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Fit a linear regression model where you describe satisfaction with the design matrix X you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression().fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. predict and compute RMSE on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for linear regression for training data is : 1.6787746754375492\n"
     ]
    }
   ],
   "source": [
    "yhat_train = regr.predict(X_train)\n",
    "rmse_train = np.sqrt(np.mean((y_train - yhat_train)**2))\n",
    "print(\"RMSE for linear regression for training data is :\", rmse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. predict and compute RMSE on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for linear regression for testing data is : 1.694504997938269\n"
     ]
    }
   ],
   "source": [
    "yhat_test = regr.predict(X_test)\n",
    "rmse_test = np.sqrt(np.mean((y_test - yhat_test)**2))\n",
    "print(\"RMSE for linear regression for testing data is :\", rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Repeat the previous with Ridge regression, play a little with different alpha-s. Which if gave you the best testing RMSE? (No need for a rigorous analysis, just play a little)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Ridge for alpha 0.1 is : 1.6944995638586497\n",
      "RMSE for Ridge for alpha 0.5 is : 1.6944812870064208\n",
      "RMSE for Ridge for alpha 1 is : 1.6944642122118978\n",
      "RMSE for Ridge for alpha 3 is : 1.6944269656112967\n",
      "RMSE for Ridge for alpha 10 is : 1.6943819455789368\n",
      "RMSE for Ridge for alpha 100 is : 1.6949582826433416\n"
     ]
    }
   ],
   "source": [
    "#large alpha = large penalty\n",
    "alphas= [0.1,0.5,1,3, 10,100]\n",
    "\n",
    "for alpha in alphas:\n",
    "    mr = Ridge(alpha=alpha).fit(X_train, y_train)\n",
    "    yhatr = mr.predict(X_test)\n",
    "    rmser = np.sqrt(np.mean((y_test - yhatr)**2))\n",
    "    print(\"RMSE for Ridge for alpha\" , alpha ,\"is :\", rmser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can see that a alpha of 3 gave us the lowest RMSE for the model using Ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. And repeat with Lasso regression again playing a little with different alpha-s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Lasso for alpha 0.1 is : 1.7521929138180097\n",
      "RMSE for Lasso for alpha 0.5 is : 1.9037143888620092\n",
      "RMSE for Lasso for alpha 1 is : 1.9670724526062617\n",
      "RMSE for Lasso for alpha 3 is : 2.2354242613837343\n",
      "RMSE for Lasso for alpha 10 is : 2.2354242613837343\n",
      "RMSE for Lasso for alpha 100 is : 2.2354242613837343\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.1,0.5,1,3,10,100]\n",
    "for alpha in alphas:\n",
    "    ml = Lasso(alpha=alpha).fit(X_train, y_train)\n",
    "    yhatl = ml.predict(X_test)\n",
    "    rmsel = np.sqrt(np.mean((y_test - yhatl)**2))\n",
    "    print(\"RMSE for Lasso for alpha\", alpha, \"is :\", rmsel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can see that the smallest alpha value of 0.1 gave us the lowest RMSE for the model using Lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. comment your results:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Compare RMSE on testing/training data. What does this suggest in terms of overfitting?\n",
    "\n",
    "Answer: The model has a similar RMSE on the training and test data. These means that our model has a good fit and can predict correct values on unseen data as well and has not overfit our training data.\n",
    "\n",
    "(b) Compare RMSE for OLS, Ridge and Lasso\n",
    "\n",
    "Answer: The RMSE for OLS, Ridge and Lasso are almost similar in value. This shows that regularization did not add any value to as our initial model did not overfit the data.\n",
    "\n",
    "(c) Compare the resulting RMSE with the small benchmark model you did above\n",
    "\n",
    "Answer: By comparing the RMSE of the small benchmark model and of models above, we can see that there is a very minute difference between the RMSE's of the model with 3 predictors and the model with over 150 predictors. Thus we can say that the extra 150 variables added very little explanatory power to the model and were not relevant for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Let's Overfit!\n",
    "\n",
    "As WVS is a relatively large dataset we cannot easily overfit by adding more variables. But we can go another easy route instead: we take a subsample.\n",
    "\n",
    "1. Create a subsample of your design matrix and the outcome variable. Choose a large-ish sample that overfits. The size depends on which variables do you exactly choose, in my case 2000 obs rarely overfits (it depends on the train-validation split), 1000 typically overfits.\n",
    "2. repeat the steps you did above.\n",
    "3. comment how do OLS, Ridge, Lasso perform on testing/training in case of overfitting.\n",
    "4. comment the condition number of design matrix and overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs_sample_df = pd.concat([design_matrix_df, y_res] , axis = 1)\n",
    "wvs_sample_df = wvs_sample_df.sample(n = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. repeat the steps you did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_model(X , y):\n",
    "    print(\"Condition number of X is :\", np.linalg.cond(X))\n",
    "    # Splitting the data into training and testing data \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)  \n",
    "    print(\"Condition number of X-train is :\", np.linalg.cond(X_train))\n",
    "    regr = LinearRegression().fit(X_train, y_train) \n",
    "    #print(\"sklearn estimated coefficients:\\n\", regr.coef_)\n",
    "    yhat_train = regr.predict(X_train)\n",
    "    yhat_test = regr.predict(X_test)\n",
    "    rmse_train = np.sqrt(np.mean((y_train - yhat_train)**2))\n",
    "    rmse_test = np.sqrt(np.mean((y_test - yhat_test)**2))\n",
    "    print(\"\\nRMSE for linear regression for training data is :\", rmse_train)\n",
    "    print(\"RMSE for linear regression for testing data is :\", rmse_test)\n",
    "    \n",
    "    alphas= [0.1,0.5,1,3,10,100]  # sometimes refer to this as lambda\n",
    "    for alpha in alphas:\n",
    "        mr = Ridge(alpha=alpha).fit(X_train, y_train)\n",
    "        yhatr_train = mr.predict(X_train)\n",
    "        yhatr_test = mr.predict(X_test)\n",
    "        rmser_train = np.sqrt(np.mean((y_train - yhatr_train)**2))\n",
    "        rmser_test = np.sqrt(np.mean((y_test - yhatr_test)**2))\n",
    "        print(\"\\nRMSE for Ridge for alpha \", alpha, \" for training data is :\", rmser_train)\n",
    "        print(\"RMSE for Ridge for alpha \", alpha, \" for testing data is :\", rmser_test)  \n",
    "   \n",
    "    for alpha in alphas:\n",
    "        ml = Lasso(alpha=alpha).fit(X_train, y_train)\n",
    "        yhatl_train = ml.predict(X_train)\n",
    "        yhatl_test = ml.predict(X_test)\n",
    "        rmsel_train = np.sqrt(np.mean((y_train - yhatl_train)**2))\n",
    "        rmsel_test = np.sqrt(np.mean((y_test - yhatl_test)**2))\n",
    "        print(\"\\nRMSE for Lasso for alpha \", alpha, \" for training data is :\", rmsel_train)\n",
    "        print(\"RMSE for Lasso for alpha \", alpha, \" for testing data is :\", rmsel_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number of X is : 5219.945188462191\n",
      "Condition number of X-train is : 4944.429146593619\n",
      "\n",
      "RMSE for linear regression for training data is : 1.4373454541904003\n",
      "RMSE for linear regression for testing data is : 1.9493014818979681\n",
      "\n",
      "RMSE for Ridge for alpha  0.1  for training data is : 1.438122347250804\n",
      "RMSE for Ridge for alpha  0.1  for testing data is : 1.954106902236144\n",
      "\n",
      "RMSE for Ridge for alpha  0.5  for training data is : 1.4396086704188784\n",
      "RMSE for Ridge for alpha  0.5  for testing data is : 1.9552118772770417\n",
      "\n",
      "RMSE for Ridge for alpha  1  for training data is : 1.4407819854559494\n",
      "RMSE for Ridge for alpha  1  for testing data is : 1.953658888701536\n",
      "\n",
      "RMSE for Ridge for alpha  3  for training data is : 1.4465409559530011\n",
      "RMSE for Ridge for alpha  3  for testing data is : 1.9483956059535357\n",
      "\n",
      "RMSE for Ridge for alpha  10  for training data is : 1.4663478024140209\n",
      "RMSE for Ridge for alpha  10  for testing data is : 1.9397651077789044\n",
      "\n",
      "RMSE for Ridge for alpha  100  for training data is : 1.5289267207658723\n",
      "RMSE for Ridge for alpha  100  for testing data is : 1.9044070594638558\n",
      "\n",
      "RMSE for Lasso for alpha  0.1  for training data is : 1.6510210761846793\n",
      "RMSE for Lasso for alpha  0.1  for testing data is : 1.8077082665621\n",
      "\n",
      "RMSE for Lasso for alpha  0.5  for training data is : 1.7747751251729464\n",
      "RMSE for Lasso for alpha  0.5  for testing data is : 1.9052527161439925\n",
      "\n",
      "RMSE for Lasso for alpha  1  for training data is : 1.879613706411209\n",
      "RMSE for Lasso for alpha  1  for testing data is : 1.982856223357742\n",
      "\n",
      "RMSE for Lasso for alpha  3  for training data is : 2.183802589521315\n",
      "RMSE for Lasso for alpha  3  for testing data is : 2.2508734415777356\n",
      "\n",
      "RMSE for Lasso for alpha  10  for training data is : 2.183802589521315\n",
      "RMSE for Lasso for alpha  10  for testing data is : 2.2508734415777356\n",
      "\n",
      "RMSE for Lasso for alpha  100  for training data is : 2.183802589521315\n",
      "RMSE for Lasso for alpha  100  for testing data is : 2.2508734415777356\n"
     ]
    }
   ],
   "source": [
    "ml_model(wvs_sample_df.loc[:, wvs_sample_df.columns != 'V23'], wvs_sample_df.V23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Comment how do OLS, Ridge, Lasso perform on testing/training in case of overfitting.\n",
    "\n",
    "Answer:\n",
    "\n",
    "On overfitting the data we have the below observations for all the models:\n",
    "\n",
    "1) OLS: The RMSE on the training data is lower than that of the testing data. This means that our model is overfitting our trainng data as it cannot predict correct values on unseen data.\n",
    "\n",
    "2) Ridge: For alpha of 100, we can see that the RMSE of the test data is the lowest but it is still a little higher than that of the training data. However, this is lower than that of the OLS model. This shows that using regularization we can reduce the overfitting of the model caused by simple linear regression.\n",
    "\n",
    "3) Lasso: For alpha of 0.1, we can see that the RMSE of the test data is the lowest but it is still higher than that of the training data. However, this is lower than that of the OLS model. This shows that using regularization we can reduce the overfitting of the model caused by simple linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
